# 参数：{{._extra.project}} {{._extra.name}}[由tenant-model构成] 
# {{._extra.container_image}} {{._extra.memory}} {{._extra.cpu_num}} {{._extra.gpu_num}} {{._extra.model_load_time}} {{._extra.device}} {{._extra.shm_size}}[默认64M] 
# {{._extra.replicas}} {{._extra.model_path}} {{._extra.model_server}} {{._extra.memory_r}} {{._extra.cpu_num_r}} {{._extra.gpu_num_r}}
# 自定义部署
apiVersion: v1
kind: PersistentVolume
metadata:
  name: {{._extra.project}}-{{._extra.name}}-deploy-pv-{{._extra.timestamp}}
  namespace: seldon
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Delete
  nfs:
    path: {{._extra.model_path}}
    server: {{._extra.model_server}}
  mountOptions:
    - "nolock"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{._extra.project}}-{{._extra.name}}-deploy-pvc-{{._extra.timestamp}}
  namespace: seldon
  labels:
    randomVersion: {{._extra.randomVersion}}
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  volumeName: {{._extra.project}}-{{._extra.name}}-deploy-pv-{{._extra.timestamp}}
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: {{._extra.project}}-{{._extra.name}}
  namespace: seldon
  labels:
    randomVersion: {{._extra.randomVersion}}
spec:
  gateways:
  - istio-system/seldon-gateway
  hosts:
  - '*'
  http:
  - match:
    - uri:
        prefix: /seldon/seldon/{{._extra.project}}-{{._extra.name}}/
    rewrite:
      uri: /
    route:
    - destination:
        host: {{._extra.project}}-{{._extra.name}}-default.seldon.svc.cluster.local
        port:
          number: 8000
---
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: {{._extra.project}}-{{._extra.name}}
  namespace: seldon
  labels: 
    task-project: "{{._extra.projectName}}"
    task-user: "{{._extra.ownerName}}"
    task-run-id: "{{._instance.ID}}"
    task-id: "{{._task.ID}}"
    task-name: "{{._task.Name}}"
    task-uuid: "{{._task.UUID}}"
    taskd: taskd
    randomVersion: {{._extra.randomVersion}}
spec:
  annotations:
    seldon.io/engine-seldon-log-messages-externally: "true"
  name: {{._extra.project}}-{{._extra.name}}
  predictors:
    - componentSpecs:
        - spec:
            nodeSelector:
               nodeId: {{._extra.node_id}}
            containers:
              - name: {{._extra.project}}-{{._extra.name}}
                image: {{._extra.container_image}}
                command: ["/bin/bash", "-c"]
                args:
                - |
                  source /miniconda/etc/profile.d/conda.sh
                  conda activate default_env
                  seldon-core-microservice Infer
                  if [ $? != 0 ]; then
                    echo "ERROR: 模型部署失败，请重新部署" >&1
                    exit 1
                  fi
                volumeMounts:
                  - name: {{._extra.project}}-{{._extra.name}}-provision-location
                    mountPath: /workspace/model_files  # 挂载到指定路径，后续规定用户必须按照标准上传模型
                    subPath: model_files
                    readOnly: true
                  - name: {{._extra.project}}-{{._extra.name}}-deploy-shm-size
                    mountPath: /dev/shm
                env:
                - name: pod_name
                  value: {{._extra.project}}-{{._extra.name}}
                - name: TRACING
                  value: '1'
                - name: SELDON_DEBUG
                  value: 'true'
                - name: GRPC_WORKERS
                  value: '0'
                resources:
                  requests:
                    memory: {{._extra.memory_r}}Gi      # 一般设置为limits的一半，用户输入的值为limits
                    cpu: {{._extra.gpu_num_r}}
                    nvidia.com/gpu: {{._extra.gpu_num_r}}
                  limits:
                    memory: {{._extra.memory}}Gi
                    cpu: {{._extra.cpu_num}}
                    nvidia.com/gpu: {{._extra.gpu_num}}
                livenessProbe:
                  initialDelaySeconds: {{._extra.model_load_time}}  # 加载模型的时间
                  periodSeconds: 5
                  httpGet:
                    path: /health/status
                    port: 9000
                readinessProbe:
                  initialDelaySeconds: {{._extra.model_load_time}}
                  periodSeconds: 5
                  httpGet:
                    path: /health/status
                    port: 9000
            volumes:
              - name: {{._extra.project}}-{{._extra.name}}-provision-location
                persistentVolumeClaim:
                  claimName: {{._extra.project}}-{{._extra.name}}-deploy-pvc-{{._extra.timestamp}}
              - name: {{._extra.project}}-{{._extra.name}}-deploy-shm-size
                emptyDir:
                  medium: Memory
                  sizeLimit: {{._extra.shm_size}}Mi  # 默认值64M ≈ 0.064
            initContainers:
              - name: {{._extra.project}}-{{._extra.name}}-model-initializer
                image: ubuntu:20.04
                imagePullPolicy: IfNotPresent

      graph:
        logger:
          mode: all
        name: {{._extra.project}}-{{._extra.name}}
        type: MODEL
        parameters: [
          {
            "name": "base_path",
            "type": "STRING",
            "value": "/workspace"
          }
        ]
      name: default
      replicas: {{._extra.replicas}}

